{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hC-MHA4AfV7",
        "outputId": "83adadce-c72c-448a-c9cb-fccaad3b8639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
            "    from pip._internal.cli.parser import ConfigOptionParser\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
            "    from pip._internal.configuration import Configuration, ConfigurationError\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/configuration.py\", line 21, in <module>\n",
            "    from pip._internal.exceptions import (\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/exceptions.py\", line 8, in <module>\n",
            "    from pip._vendor.requests.models import Request, Response\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/requests/__init__.py\", line 43, in <module>\n",
            "    from pip._vendor import urllib3\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/__init__.py\", line 13, in <module>\n",
            "    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/connectionpool.py\", line 11, in <module>\n",
            "    from .connection import (\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/connection.py\", line 15, in <module>\n",
            "    from .util.proxy import create_proxy_ssl_context\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/util/__init__.py\", line 8, in <module>\n",
            "    from .ssl_ import (\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/util/ssl_.py\", line 17, in <module>\n",
            "    from .url import BRACELESS_IPV6_ADDRZ_RE, IPV4_RE\n",
            "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/util/url.py\", line 71, in <module>\n",
            "    _HOST_PORT_RE = re.compile(_HOST_PORT_PAT, re.UNICODE | re.DOTALL)\n",
            "  File \"/usr/local/lib/python3.7/re.py\", line 236, in compile\n",
            "    return _compile(pattern, flags)\n",
            "  File \"/usr/local/lib/python3.7/re.py\", line 288, in _compile\n",
            "    p = sre_compile.compile(pattern, flags)\n",
            "  File \"/usr/local/lib/python3.7/sre_compile.py\", line 764, in compile\n",
            "    p = sre_parse.parse(p, flags)\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 924, in parse\n",
            "    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 810, in _parse\n",
            "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 810, in _parse\n",
            "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 810, in _parse\n",
            "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 523, in _parse\n",
            "    this = sourceget()\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 256, in get\n",
            "    self.__next()\n",
            "  File \"/usr/local/lib/python3.7/sre_parse.py\", line 234, in __next\n",
            "    index = self.index\n",
            "KeyboardInterrupt\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch==1.12.1+cu116 in /usr/local/lib/python3.7/site-packages (1.12.1+cu116)\n",
            "Requirement already satisfied: torchvision==0.13.1+cu116 in /usr/local/lib/python3.7/site-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: torchaudio==0.12.1 in /usr/local/lib/python3.7/site-packages (0.12.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /root/.local/lib/python3.7/site-packages (from torch==1.12.1+cu116) (4.7.1)\n",
            "Requirement already satisfied: numpy in /root/.local/lib/python3.7/site-packages (from torchvision==0.13.1+cu116) (1.21.6)\n",
            "Requirement already satisfied: requests in /root/.local/lib/python3.7/site-packages (from torchvision==0.13.1+cu116) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/.local/lib/python3.7/site-packages (from torchvision==0.13.1+cu116) (9.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/.local/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu116) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.local/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu116) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/.local/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu116) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/.local/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu116) (3.7)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/eriklindernoren/PyTorch-YOLOv3\n",
        "# %cd PyTorch-YOLOv3/\n",
        "# !pip3 install poetry --user\n",
        "# !poetry install\n",
        "# !./weights/download_weights.sh\n",
        "# !pip3 install pytorchyolo --user\n",
        "# !poetry run yolo-test --weights weights/yolov3.weights\n",
        "\n",
        "!pip uninstall -y torch torchvision\n",
        "!pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorchyolo.models import load_model_own\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/app/src\n"
          ]
        }
      ],
      "source": [
        "%cd /app/src\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pytorchyolo import detect, models\n",
        "\n",
        "from utils.viz_utils import draw_bounding_boxes\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "def include_utils():\n",
        "    parent_directory = os.path.abspath('/app/utils')\n",
        "    sys.path.append(parent_directory)\n",
        "\n",
        "include_utils()\n",
        "\n",
        "\n",
        "# Load the YOLO model\n",
        "model = models.load_model(\n",
        "  \"/app/PyTorch-YOLOv3/config/yolov3.cfg\",\n",
        "  \"/app/bin/model_files/yolov3.weights\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jFmcAen_ZM8",
        "outputId": "9f63a87c-7969-46e8-a210-eb279d5ef64c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the image as a numpy array\n",
        "img = cv2.imread(\"/app/data/images/embedding_tester/apple_2.jpg\")\n",
        "\n",
        "with open(\"/app/PyTorch-YOLOv3/data/coco.names\", 'r') as rf:\n",
        "    classes = rf.read().split('\\n')\n",
        "\n",
        "# Convert OpenCV bgr to rgb\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Runs the YOLO model on the image\n",
        "boxes = detect.detect_image(model, img)\n",
        "\n",
        "draw_bounding_boxes(img, boxes, classes)\n",
        "\n",
        "# for box in boxes:\n",
        "#   img = cv2.rectangle(img, box[:2].astype(int), box[2:4].astype(int), (0,0,0), 10)\n",
        "\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>\n",
        "<hr>\n",
        "\n",
        "# storage\n",
        "\n",
        "<hr>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>\n",
        "<hr>\n",
        "<h1>EXPERIMENTS WITH MODIFIED SOURCE HERE ON</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /app/src\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pytorchyolo import detect, models\n",
        "\n",
        "from utils.viz_utils import draw_bounding_boxes\n",
        "\n",
        "import sys\n",
        "\n",
        "with open(\"/app/PyTorch-YOLOv3/data/coco.names\", 'r') as rf:\n",
        "    classes = rf.read().split('\\n')\n",
        "    \n",
        "def include_modules(libdir):\n",
        "    parent_directory = os.path.abspath(libdir)\n",
        "    sys.path.append(parent_directory)\n",
        "\n",
        "include_modules('/app/src/utils')\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Load the YOLO model\n",
        "model = models.load_model_own(\n",
        "  \"/app/PyTorch-YOLOv3/config/yolov3.cfg\",\n",
        "  \"/app/bin/model_files/yolov3.weights\").to(device)\n",
        "\n",
        "\n",
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pytorchyolo.models import load_model\n",
        "from pytorchyolo.utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
        "from pytorchyolo.utils.datasets import ImageFolder\n",
        "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.ticker import NullLocator\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size=416\n",
        "conf_thres=0.5\n",
        "nms_thres=0.5\n",
        "\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "\n",
        "img = cv2.imread(\"/app/data/images/embedding_tester/apple_2.jpg\")\n",
        "\n",
        "\n",
        "# Convert OpenCV bgr to rgb\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "# Configure input\n",
        "input_img = transforms.Compose([\n",
        "    DEFAULT_TRANSFORMS,\n",
        "    Resize(img_size)])(\n",
        "        (img, np.zeros((1, 5))))[0].unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_img = input_img.to(\"cuda\")\n",
        "\n",
        "# Get detections\n",
        "with torch.no_grad():\n",
        "    detections = model(input_img)\n",
        "    # detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
        "    # detections = rescale_boxes(detections[0], img_size, img.shape[:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.yolo_utils import yolo_model\n",
        "from utils.viz_utils import draw_bounding_boxes\n",
        "\n",
        "\n",
        "test_yolo = yolo_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "boxes : [[116.08516 150.97758 387.11856 238.94258]\n",
            " [390.05267 239.07567 538.6677  378.15506]]\n",
            " object embedding_locations : [[ 4  4  7  7]\n",
            " [ 8  8 11 11]]\n",
            "boxes : [[143.6355  272.41232 270.64465 397.59027]]\n",
            " object embedding_locations : [[ 8  3 11  6]]\n",
            "boxes : [[ 64.03949 302.52893 201.40988 425.     ]]\n",
            " object embedding_locations : [[10  1 13  4]]\n",
            "boxes : [[133.48222 134.94226 274.91492 260.61563]]\n",
            " object embedding_locations : [[5 4 8 7]]\n",
            "boxes : [[451.09494 274.66785 593.2746  426.70435]\n",
            " [113.97594 279.29468 245.0633  414.5993 ]]\n",
            " object embedding_locations : [[ 9  9 12 12]\n",
            " [ 9  2 12  5]]\n",
            "boxes : [[385.3665  286.03903 524.2553  427.64282]\n",
            " [155.627   236.00618 352.54803 352.12186]]\n",
            " object embedding_locations : [[ 8  8 11 11]\n",
            " [ 6  4  9  7]]\n",
            "boxes : [[  8.050338 282.29752  207.19604  427.      ]]\n",
            " object embedding_locations : [[ 9  1 12  4]]\n",
            "boxes : [[274.91138 197.28207 633.5343  457.9926 ]]\n",
            " object embedding_locations : [[ 7  8 10 11]]\n",
            "boxes : [[ 62.113834   0.       579.10724  298.414   ]]\n",
            " object embedding_locations : [[3 5 6 8]]\n",
            "boxes : [[193.42125  105.69135  308.14124  464.10416 ]\n",
            " [429.2778    75.679756 630.1902   327.1549  ]]\n",
            " object embedding_locations : [[ 6  4  9  7]\n",
            " [ 4  9  7 12]]\n",
            "boxes : [[164.26192  214.05847  309.23145  306.40195 ]\n",
            " [ 10.483861 230.19687  163.44292  334.8828  ]]\n",
            " object embedding_locations : [[ 9  5 12  8]\n",
            " [ 9  1 12  4]]\n",
            "boxes : [[257.97574    75.70976   604.00574   384.48352  ]\n",
            " [113.2258     41.17805   244.34369   185.6388   ]\n",
            " [  4.3197103 283.47733   200.23083   427.       ]]\n",
            " object embedding_locations : [[ 5  7  8 10]\n",
            " [ 2  2  5  5]\n",
            " [ 9  1 12  4]]\n",
            "boxes : [[ 27.251318 204.0967   302.32953  395.1564  ]\n",
            " [427.06213  213.3539   565.3788   329.29196 ]]\n",
            " object embedding_locations : [[ 8  2 11  5]\n",
            " [ 7  9 10 12]]\n",
            "boxes : [[409.4659  185.16766 638.2703  411.9624 ]\n",
            " [159.29839 149.64745 293.34073 291.3162 ]]\n",
            " object embedding_locations : [[ 8  9 11 12]\n",
            " [ 5  3  8  6]]\n",
            "boxes : [[  0.          0.        232.57967   427.       ]\n",
            " [303.10107     7.9807696 638.73724   136.07285  ]]\n",
            " object embedding_locations : [[ 5  1  8  4]\n",
            " [ 1  8  4 11]]\n",
            "boxes : [[166.97334 192.77148 307.77017 352.3237 ]\n",
            " [  0.      369.27524 178.67006 480.     ]]\n",
            " object embedding_locations : [[ 6  3  9  6]\n",
            " [10  0 13  3]]\n",
            "boxes : [[163.69638   23.009045 477.99557  333.      ]\n",
            " [  9.677245 180.53078  130.12987  283.76712 ]]\n",
            " object embedding_locations : [[ 5  7  8 10]\n",
            " [ 7  0 10  3]]\n",
            "boxes : [[163.8132    19.051186 308.48013  480.      ]\n",
            " [344.34848   34.520473 520.10187  480.      ]]\n",
            " object embedding_locations : [[ 5  3  8  6]\n",
            " [ 5  7  8 10]]\n",
            "boxes : [[304.43912     4.6119804 462.8125    480.       ]\n",
            " [473.97464   169.56926   633.9864    445.5631   ]]\n",
            " object embedding_locations : [[ 5  6  8  9]\n",
            " [ 7 10 10 13]]\n",
            "boxes : [[ 65.90204 149.41211 349.38318 292.81418]\n",
            " [493.6198  149.40443 629.36163 266.98853]]\n",
            " object embedding_locations : [[ 4  3  7  6]\n",
            " [ 4 10  7 13]]\n",
            "releasing video\n",
            "\n",
            "saved file name : 36bd30c588fb5e76bec069b8d0059100_yolo_numims_20_embwin_3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "img_size=416\n",
        "conf_thres=0.5\n",
        "nms_thres=0.01\n",
        "\n",
        "image_paths = [os.path.join(\"/app/data/images/embedding_tester/\", im_name) for im_name in os.listdir(\"/app/data/images/embedding_tester/\")]\n",
        "# img = cv2.imread(\"/app/data/images/embedding_tester/apple_3.jpg\")\n",
        "\n",
        "embedding_list = []\n",
        "# for impath in image_paths:\n",
        "# impath = image_paths[4] \n",
        "# img = cv2.imread(impath)\n",
        "embedding_details, embeddings_matrix = test_yolo.get_embeddings(image_paths[:20],\n",
        "            img_size=img_size,\n",
        "            conf_thres=conf_thres,\n",
        "            nms_thres=nms_thres,\n",
        "            head = 0,\n",
        "            embedding_window_size = 3,\n",
        "            viz = False)\n",
        "# print(embeddings[0].shape)\n",
        "# embedding_list+=embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_details['source_paths']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testim = cv2.imread(image_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a = []\n",
        "a += [\"impat\" for i in range(10)]\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "pad embeddings which dont have 5x5\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "a = embedding_list[0]\n",
        "for i in range(1,len(embedding_list)):\n",
        "    print(f\" a shape : {a.shape} , embedding_listp[i] shape : {embedding_list[i].shape}\")\n",
        "    a = np.concatenate([a, embedding_list[i]])\n",
        "\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(detections[0])\n",
        "print(f\"num layer outputs : {len(detections[0])}\\n\")\n",
        "print(f\"num heads = {len(detections[1])}\\n\")\n",
        "for ind, i in enumerate(model.module_defs) :\n",
        "    if i['type'] == 'yolo':\n",
        "        print(f\"\\nlayer {ind} : name : {i['type']}\\n\") \n",
        "        print(\" details : \\n\")\n",
        "        pprint(i)\n",
        "        print(\"\\n\")\n",
        "    else:\n",
        "        print(f\"layer {ind} : name : {i['type']}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_layer_details(model, detections, layer_num):\n",
        "#     print(\"layer details : \\n\")\n",
        "#     pprint(model.module_defs[layer_num])\n",
        "#     print(f\"\\noutput shape : {detections[0][layer_num].shape}\")\n",
        "\n",
        "\n",
        "# img_size=416\n",
        "# conf_thres=0.5\n",
        "# nms_thres=0.5\n",
        "\n",
        "# for ind, head_ind in enumerate([82, 94, 106]):\n",
        "#     img = cv2.imread(\"/app/data/images/embedding_tester/apple_2.jpg\")   \n",
        "#     get_layer_details(model, detections, head_ind)\n",
        "#     head_detections = detections[0][head_ind]\n",
        "#     print(f\"head {ind + 1} : total boxes detected  : {head_detections.shape}\")\n",
        "#     head_detections = non_max_suppression(head_detections, conf_thres, nms_thres)\n",
        "#     print(f\"num boxes post supression : {head_detections[0].shape}\")\n",
        "#     head_detections = rescale_boxes(head_detections[0], img_size, img.shape[:2])\n",
        "#     boxes = head_detections.numpy()\n",
        "#     image = draw_bounding_boxes(img, boxes, classes)\n",
        "#     plt.imshow(image[:, :, ::-1])\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### the 83nd , 95th and 107th layers are heads <br>\n",
        "\n",
        "### we will analyse the emebddings corresponding to each of the layers 1 by 1.<br>\n",
        "\n",
        "### starting with the 1st head : 83rd layer\n",
        "\n",
        "#### the heads are actually just reshaping layer, that apply the final scaling scaling transforms to convert logits to xyxy conf cls\n",
        "#### the layers before heads are fully connected linear layers, so these are the ones that actually do the classification and regresssions\n",
        "#### Therefore, to extract the embeddings we take the the 2nd predecessor of a head, so here, we take 81st layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_layer_details(model, detections, layer_ind):\n",
        "    print(f\"embeddings of layer : {layer_ind}:\\n\")\n",
        "    pprint(model.module_defs[layer_ind])\n",
        "    print(f\"\\noutputs shape : {detections[0][layer_ind].shape}\")\n",
        "\n",
        "    \n",
        "\n",
        "get_layer_details(model, detections, 80)\n",
        "\n",
        "raw_embeddings = detections[0][80]\n",
        "outputs = model.module_list[81](raw_embeddings)\n",
        "\n",
        "raw_embeddings.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in model.module_defs:\n",
        "    print(i[\"type\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def raw_predict(img, input_img_size, raw_outputs, conf_thres, nms_thres, viz = False, reverse_img_for_viz = True):\n",
        "    head_detections = non_max_suppression(raw_outputs, conf_thres, nms_thres)\n",
        "    # print(f\"num boxes post supression : {head_detections[0].shape}\")\n",
        "    head_detections = rescale_boxes(head_detections[0], input_img_size, img.shape[:2])\n",
        "    boxes = head_detections.numpy()\n",
        "    image = draw_bounding_boxes(img, boxes, classes)\n",
        "    if reverse_img_for_viz:\n",
        "        image = image[:, :, ::-1]\n",
        "    if viz:\n",
        "        plt.imshow()\n",
        "        plt.show()\n",
        "    return image, boxes    \n",
        "\n",
        "def predict_from_embedding(raw_embeddings,\n",
        "                           model,\n",
        "                           embeddding_layer_ind,\n",
        "                           input_img_size,\n",
        "                           conf_thres,\n",
        "                           nms_thres,\n",
        "                           viz = False,\n",
        "                           reverse_img_for_viz = True):\n",
        "    outputs = model.module_list[embeddding_layer_ind + 1](raw_embeddings)\n",
        "    outputs = model.module_list[embeddding_layer_ind + 2][0](outputs, img_size)\n",
        "    image = raw_predict(img, input_img_size, outputs, conf_thres, nms_thres, viz, reverse_img_for_viz)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EMBEDDING OCCULSION VISUALIZATION VIDEO GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from utils.viz_utils import create_frame\n",
        "\n",
        "# #XXXXXXXXXXXXXXXXXXXXXXXXXX     video details\n",
        "# # frames,\n",
        "# # output_path = '/app/bin/outputs/embedding_occulsion_analysis_head_1.mp4'\n",
        "# output_path = '/app/bin/outputs/temp.mp4'\n",
        "# fps=5\n",
        "# frame_size=(800, 400)\n",
        "\n",
        "# frame_counter = 0\n",
        "\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using 'mp4v' for compatibility\n",
        "# video = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
        "# #XXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "\n",
        "\n",
        "\n",
        "# embedding_ind = 80 # head 1\n",
        "\n",
        "\n",
        "# num_rows, num_cols = raw_embeddings.shape[-2:]\n",
        "# for window_size in range(1,int(round(2*num_rows/3))):\n",
        "#     for i in range(num_rows-window_size):\n",
        "#         for j in range(num_cols-window_size):\n",
        "\n",
        "#             img = cv2.imread(\"/app/data/images/embedding_tester/apple_2.jpg\")   \n",
        "\n",
        "#             img_size = input_img.size(2)\n",
        "#             raw_embeddings = detections[0][embedding_ind].clone()\n",
        "\n",
        "#             raw_embeddings[:,:,i:i+window_size,j:j+window_size] = 0\n",
        "\n",
        "#             viz_image = predict_from_embedding(raw_embeddings,\n",
        "#                                     model,\n",
        "#                                     embedding_ind,\n",
        "#                                     img_size,\n",
        "#                                     conf_thres,\n",
        "#                                     nms_thres)\n",
        "            \n",
        "#             raw_embeddings[0, 0, :, :].detach().cpu().numpy()\n",
        "\n",
        "            \n",
        "#             frame = create_frame([raw_embeddings[0, 0, :, :].detach().cpu().numpy(), viz_image], [\"embedding\", \"results\"], figsize=(10, 5))\n",
        "            \n",
        "#             resized_frame = cv2.resize(frame, frame_size)  \n",
        "#             video.write(resized_frame)\n",
        "\n",
        "#             frame_counter+=1\n",
        "#             if frame_counter % 30 == 0:\n",
        "#                 print(f\"{frame_counter} frames over\")\n",
        "                \n",
        "# video.release()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "getting pred cell memberships\n",
        "\"\"\"\n",
        "embeddding_layer_ind = 80\n",
        "input_img_size = img_size\n",
        "raw_embeddings = detections[0][embeddding_layer_ind]\n",
        "\n",
        "outputs = model.module_list[embeddding_layer_ind + 1](raw_embeddings)\n",
        "outputs = model.module_list[embeddding_layer_ind + 2][0](outputs, img_size)\n",
        "head_detections = non_max_suppression(outputs, conf_thres, nms_thres)\n",
        "head_detections = rescale_boxes(head_detections[0], input_img_size, img.shape[:2])\n",
        "boxes = head_detections.numpy()\n",
        "\n",
        "vizimg = img.copy()\n",
        "\n",
        "vizimg = draw_bounding_boxes(vizimg, boxes, classes)\n",
        "plt.imshow(vizimg)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vizimg = img.copy()\n",
        "batch_size, num_channels , grid_num_rows, grid_num_cols = detections[0][embeddding_layer_ind].shape \n",
        "\n",
        "grid_dim_row = int(round(img.shape[0] / grid_num_rows))\n",
        "grid_dim_col = int(round(img.shape[1] / grid_num_cols))\n",
        "grid_dims = np.array([grid_dim_col, grid_dim_row])\n",
        "print(grid_dims)\n",
        "centers = (boxes[:, 2:4] - boxes[: , :2])/2 + boxes[: , :2]\n",
        "mem_starts_xy = centers // grid_dims # we need xlength , ylength, not row_length, col_length\n",
        "mem_cells_mins = mem_starts_xy * grid_dims\n",
        "print(mem_cells_mins)\n",
        "mem_cells_maxs = mem_cells_mins + grid_dims\n",
        "print(mem_cells_maxs)\n",
        "# mem_cell_boxes = np.concatenate([centers, mem_cells_maxs], axis = 1).astype(int)\n",
        "mem_cell_boxes = np.concatenate([mem_cells_mins, mem_cells_maxs], axis = 1).astype(int)\n",
        "\n",
        "for center in centers.astype(int):\n",
        "    vizimg = cv2.circle(vizimg, (center[0], center[1]), 10, (0,0,0), 17)\n",
        "    vizimg = cv2.circle(vizimg, (center[0], center[1]), 10, (255,255,255), 3)\n",
        "\n",
        "for xmin, ymin ,xmax, ymax in mem_cell_boxes.astype(int):\n",
        "    vizimg = cv2.rectangle(vizimg, (xmin, ymin), (xmax, ymax), (0,0,255), 17)\n",
        "\n",
        "\n",
        "plt.imshow(vizimg)\n",
        "plt.show()      \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "centers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mem_starts_xy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = np.zeros_like(img)\n",
        "a[394:422, 560:588] = 250\n",
        "plt.imshow(a)\n",
        "plt.show()\n",
        "\n",
        "a = np.zeros([13,13])\n",
        "r,c = mem_starts_xy[0][::-1].astype(int)\n",
        "a[r,c] = 100\n",
        "plt.imshow(a)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detections.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(centers[:, ::-1] // grid_dims[::-1])\n",
        "print(mem_starts_xy[:, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.viz_utils import create_frame\n",
        "\n",
        "viz = True\n",
        "\n",
        "embeddding_layer_ind = 80\n",
        "\n",
        "outputs_layer = 82\n",
        "\n",
        "conf_thres=0.5 \n",
        "nms_thres=0.5\n",
        "input_img_size = 416\n",
        "\n",
        "raw_outputs = detections[0][outputs_layer]\n",
        "\n",
        "\n",
        "image, boxes = raw_predict(img, input_img_size, raw_outputs, conf_thres, nms_thres, viz = False, reverse_img_for_viz = True)\n",
        "\n",
        "batch_size, num_channels , grid_num_rows, grid_num_cols = detections[0][embeddding_layer_ind].shape \n",
        "\n",
        "grid_dim_row = int(round(img.shape[0] / grid_num_rows))\n",
        "grid_dim_col = int(round(img.shape[1] / grid_num_cols))\n",
        "grid_dims = np.array([grid_dim_col, grid_dim_row])\n",
        "\n",
        "centers = (boxes[:, 2:4] - boxes[: , :2])/2 + boxes[: , :2]\n",
        "\n",
        "# from xy to row col\n",
        "mem_starts_rowcol = centers[:, ::-1] // grid_dims[::-1]\n",
        "\n",
        "embedding_slice_min = np.maximum(mem_starts_rowcol - 2, np.zeros_like(mem_starts_rowcol)).astype(int)\n",
        "embedding_slice_max = np.minimum(mem_starts_rowcol + 3, np.ones_like(mem_starts_rowcol) * (grid_dims[::-1] )).astype(int)\n",
        "\n",
        "\n",
        "#####\n",
        "viz_embedding = np.concatenate([embedding_slice_min, embedding_slice_max], axis = 1)\n",
        "\n",
        "print(viz_embedding)\n",
        "print(mem_starts_rowcol)\n",
        "\n",
        "raw_embedding = detections[0][80].detach().cpu().numpy()\n",
        "\n",
        "# #XXXXXXXXXXXXXXXXXXXXXXXXXX     video details\n",
        "# frames,\n",
        "# output_path = '/app/bin/outputs/embedding_occulsion_analysis_head_1.mp4'\n",
        "output_path = '/app/bin/outputs/embedding_assignment_method.mp4'\n",
        "fps=0.4\n",
        "frame_size=(800, 400)\n",
        "\n",
        "frame_counter = 0\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using 'mp4v' for compatibility\n",
        "video = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
        "# #XXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "\n",
        "\n",
        "embeddings = []\n",
        "for ind, (rowmin, colmin, rowmax, colmax) in enumerate(viz_embedding.astype(int)):\n",
        "    embeddings.append(raw_embedding[:, :, rowmin:rowmax, colmin:colmax])\n",
        "\n",
        "    if viz == True:\n",
        "        \n",
        "        vizimg = img.copy()\n",
        "        vizimg = cv2.rectangle(vizimg, (colmin * grid_dim_row, rowmin * grid_dim_col) , (colmax * grid_dim_row , rowmax * grid_dim_col), (0,0,0) , 15)\n",
        "        vizimg = draw_bounding_boxes(vizimg, boxes[ind][None, ...], classes)\n",
        "        center_x , center_y = centers[ind].astype(int)\n",
        "        vizimg = cv2.circle(vizimg, (center_x, center_y), 20, (0,0,0), 22)\n",
        "        vizimg = cv2.circle(vizimg, (center_x, center_y), 20, (255,255,255), 5)\n",
        "\n",
        "        viz_emb = raw_embedding[0, 0, :, :].copy()\n",
        "        r, c = mem_starts_rowcol[ind].astype(int)\n",
        "        viz_emb[rowmin:rowmax, colmin:colmax] = 0.0\n",
        "        viz_emb[r,c] = 0.75\n",
        "\n",
        "            \n",
        "        frame = create_frame([viz_emb, vizimg], [\"embedding\", \"results\"], figsize=(10, 5))\n",
        "        \n",
        "        resized_frame = cv2.resize(frame, frame_size)  \n",
        "        video.write(resized_frame)\n",
        "\n",
        "        plt.imshow(frame[:, :, ::-1])\n",
        "        plt.show()\n",
        "\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detections[0][0][0][0][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = detections\n",
        "a[0][0][0][0][0][0] = 196996.3242"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mem_starts_rowcol = centers[:, ::-1] // grid_dims[::-1]\n",
        "embedding_slice_min = np.maximum(mem_starts_rowcol - 2, np.zeros_like(mem_starts_rowcol)).astype(int)\n",
        "embedding_slice_max = np.minimum(mem_starts_rowcol + 3, np.ones_like(mem_starts_rowcol) * (grid_dims[::-1] )).astype(int)\n",
        "\n",
        "viz_embedding = np.concatenate([embedding_slice_min, embedding_slice_max], axis = 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "a = np.zeros([13,13])\n",
        "(rowmin, colmin), (rowmax, colmax) = embedding_slice_min[0] , embedding_slice_max[0]\n",
        "r, c = mem_starts_rowcol[0].astype(int)\n",
        "a[rowmin:rowmax, colmin:colmax] = 50\n",
        "a[0:4,10] = 100\n",
        "a[10, 0] = 100\n",
        "plt.imshow(a)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "\n",
        "# def create_frame(images, titles, figsize=(10, 5)):\n",
        "#     layout = (1, len(images))  # Change layout to have enough spots for each image\n",
        "    \n",
        "#     total_subplots = layout[0] * layout[1]\n",
        "\n",
        "#     fig, axs = plt.subplots(layout[0], layout[1], figsize=figsize)\n",
        "#     axs = axs.flatten() if layout[0] * layout[1] > 1 else [axs]\n",
        "    \n",
        "#     # Hide any unused subplots\n",
        "#     for ax in axs[len(images):]:\n",
        "#         ax.axis('off')\n",
        "    \n",
        "#     for ax, img, title in zip(axs[:len(images)], images, titles):\n",
        "#         ax.imshow(img, aspect='auto')\n",
        "#         ax.set_title(title)\n",
        "#         ax.axis('off')\n",
        "    \n",
        "#     # plt.subplots_adjust(wspace=0.1, hspace=0.2)  # Adjust spacing to prevent overlap\n",
        "    \n",
        "#     canvas = FigureCanvas(fig)\n",
        "#     canvas.draw()\n",
        "#     frame = np.array(canvas.renderer.buffer_rgba())\n",
        "#     plt.close(fig)\n",
        "    \n",
        "#     frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)\n",
        "#     return frame\n",
        "\n",
        "\n",
        "\n",
        "# def create_video(frames, output_path='output_video.mp4', fps=10, frame_size=(1920, 1080)):\n",
        "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using 'mp4v' for compatibility\n",
        "#     video = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
        "\n",
        "#     for frame in frames:\n",
        "#         resized_frame = cv2.resize(frame, frame_size)  # Resize to ensure compatibility\n",
        "#         video.write(resized_frame)\n",
        "    \n",
        "#     video.release()\n",
        "\n",
        "# # Example usage\n",
        "# images = [np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8) for _ in range(20)]\n",
        "# titles = ['Image 1', 'Image 2', 'Image 3', 'Image 4']\n",
        "\n",
        "# frames = [create_frame([img, img],titles,  figsize=(8, 8)) for img in images]\n",
        "\n",
        "# create_video(frames, 'output_video.mp4', fps=5, frame_size=(800, 400))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
